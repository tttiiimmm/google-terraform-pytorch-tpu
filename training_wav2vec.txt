gcloud compute instances create wav2vec-vm \
--zone=europe-west4-a  \
--machine-type=n1-standard-64 \
--image-family=torch-xla \
--image-project=ml-images  \
--boot-disk-size=500GB \
--network=default \
--disk=librispeech-data \

gcloud compute tpus create wav2vec-eu-v3 \
--zone=europe-west4-a \
--network=default \
--version=pytorch-nightly \
--accelerator-type=v3-32

conda activate torch-xla-nightly

export tpu_worker=wav2vec-eu-v3
export TPU_IP_ADDRESS=10.129.80.132 \
export XRT_TPU_CONFIG="tpu_worker;0;$TPU_IP_ADDRESS:8470"

git clone -b fairseq-dev https://github.com/ultrons/fairseq.git

pip install --editable .
pip install soundfile
sudo apt-get install libsndfile1


nohup 



logfile="$(date +%Y%m%d)-wav2vec-podrun-$1.txt"

python /home/fairseq/fairseq_cli/train.py \
/home/LibriSpeech/ \
--tpu \
--bf16 \
--save-dir /home \
--max-sentences 16 \
--num-workers 6 \
--max-update 400000 \
--save-interval 1 \
--no-epoch-checkpoints \
--arch wav2vec \
--task audio_pretraining \
--lr 1e-06 \
--min-lr 1e-09 \
--optimizer adam \
--max-lr 0.005 \
--lr-scheduler cosine \
--conv-feature-layers '[(512, 10, 5), (512, 8, 4), (512, 4, 2), (512, 4, 2), (512, 4, 2), (512, 1, 1), (512, 1, 1)]' \
--conv-aggregator-layers '[(512, 2, 1), (512, 3, 1), (512, 4, 1), (512, 5, 1), (512, 6, 1), (512, 7, 1), (512, 8, 1), (512, 9, 1), (512, 10, 1), (512, 11, 1), (512, 12, 1), (512, 13, 1)]' \
--skip-connections-agg \
--residual-scale 0.5 \
--log-compression \
--warmup-updates 500 \
--warmup-init-lr 1e-07 \
--criterion binary_cross_entropy \
--num-negatives 10 \
--max-sample-size 150000 \
--max-tokens 1500000 \
--skip-invalid-size-inputs-valid-test \
--log-interval 20 \
--log-format simple \ 
--clip-norm 0 \
--disable-validation \
&> $logfile

python /home/fairseq/fairseq_cli/train.py /home/LibriSpeech/ --tpu --bf16 --save-dir /home --max-sentences 16 --num-workers 6 --max-update 400000 --save-interval 1 --no-epoch-checkpoints --arch wav2vec --task audio_pretraining --lr 1e-06 --min-lr 1e-09 --optimizer adam --max-lr 0.005 --lr-scheduler cosine --conv-feature-layers '[(512, 10, 5), (512, 8, 4), (512, 4, 2), (512, 4, 2), (512, 4, 2), (512, 1, 1), (512, 1, 1)]' --conv-aggregator-layers '[(512, 2, 1), (512, 3, 1), (512, 4, 1), (512, 5, 1), (512, 6, 1), (512, 7, 1), (512, 8, 1), (512, 9, 1), (512, 10, 1), (512, 11, 1), (512, 12, 1), (512, 13, 1)]' --skip-connections-agg --residual-scale 0.5 --log-compression --warmup-updates 500 --warmup-init-lr 1e-07 --criterion binary_cross_entropy --num-negatives 10 --max-sample-size 150000 --max-tokens 1500000 --skip-invalid-size-inputs-valid-test --log-interval 20 --log-format simple --clip-norm 0 --disable-validation 

python \
/home/fairseq/fairseq_cli/train.py \
 /home/LibriSpeech \
	 --tpu \
	 --distributed-world-size 8 \
--save-dir /home/ \
--num-workers 6 \
--bf16 \
--max-epoch 2 \
--max-sentences 16 \
--no-epoch-checkpoints \
--arch wav2vec \
--task audio_pretraining \
--lr 1e-06 \
--min-lr 1e-09 \
--optimizer adam \
--max-lr 0.005 \
--lr-scheduler cosine \
--conv-feature-layers '[(512, 10, 5), (512, 8, 4), (512, 4, 2), (512, 4, 2), (512, 4, 2), (512, 1, 1), (512, 1, 1)]' \
--conv-aggregator-layers '[(512, 2, 1), (512, 3, 1), (512, 4, 1), (512, 5, 1), (512, 6, 1), (512, 7, 1), (512, 8, 1), (512, 9, 1), (512, 10, 1), (512, 11, 1), (512, 12, 1), (512, 13, 1)]' \
--skip-connections-agg \
--residual-scale 0.5 \
--log-compression \
--warmup-updates 500 \
--warmup-init-lr 1e-07 \
--criterion binary_cross_entropy \
--num-negatives 10 \
--max-sample-size 150000 \
--max-tokens 1500000 \
--skip-invalid-size-inputs-valid-test \
--log-interval 20 \
--log-format simple \
--clip-norm 0 \
--disable-validation \
--no-save